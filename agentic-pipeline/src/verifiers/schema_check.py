"""
Verification V1: Schema Check

Validates summaries generated by Stage 1 Summarizer.
Checks for:
- Column name consistency
- Type correctness (numeric columns actually parse as numeric)
- Range sanity (non-negative yields, plausible years)
- Acceptable null rates
"""

import pandas as pd
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

import sys
sys.path.append(str(Path(__file__).parent.parent))

from utils.logging_utils import get_logger
from utils.file_utils import load_csv, load_json

logger = get_logger(__name__)


class SchemaChecker:
    """
    Verification V1: Schema validation after Stage 1.

    Validates that summaries accurately reflect the actual data.

    Example:
        >>> checker = SchemaChecker()
        >>> report = checker.verify_summary(
        ...     summary_path="data/summaries/yields.summary.json",
        ...     data_path="data/raw/yields.csv"
        ... )
        >>> print(report['status'])  # 'pass' or 'fail'
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize the Schema Checker.

        Args:
            config: Configuration dictionary
        """
        self.config = {
            'null_threshold': 0.95,
            'type_agreement_threshold': 0.95,
            'year_min': 1900,
            'year_max': datetime.now().year + 5,
            'check_column_names': True,
            'check_types': True,
            'check_ranges': True,
            'check_nulls': True
        }

        if config:
            self.config.update(config)

        logger.info("Initialized Schema Checker (V1)")

    def verify_summary(
        self,
        summary_path: Path,
        data_path: Path
    ) -> Dict[str, Any]:
        """
        Verify a summary against its source data file.

        Args:
            summary_path: Path to summary JSON
            data_path: Path to original data file

        Returns:
            Verification report dict with status, errors, and warnings
        """
        logger.info(f"Verifying summary: {summary_path.name}")

        # Load summary and data
        summary = load_json(summary_path)
        df = load_csv(data_path)

        # Initialize report
        report = {
            'summary_file': str(summary_path),
            'data_file': str(data_path),
            'timestamp': datetime.now().isoformat(),
            'status': 'pass',
            'errors': [],
            'warnings': [],
            'checks': {}
        }

        # Run checks
        if self.config['check_column_names']:
            self._check_column_names(summary, df, report)

        if self.config['check_types']:
            self._check_types(summary, df, report)

        if self.config['check_ranges']:
            self._check_ranges(summary, df, report)

        if self.config['check_nulls']:
            self._check_nulls(summary, df, report)

        # Determine overall status
        if len(report['errors']) > 0:
            report['status'] = 'fail'
        elif len(report['warnings']) > 0:
            report['status'] = 'pass_with_warnings'

        logger.info(f"  Status: {report['status']}")
        logger.info(f"  Errors: {len(report['errors'])}, Warnings: {len(report['warnings'])}")

        return report

    def _check_column_names(
        self,
        summary: Dict[str, Any],
        df: pd.DataFrame,
        report: Dict[str, Any]
    ) -> None:
        """Check that column names match exactly."""
        summary_cols = {col['name'] for col in summary['columns']}
        actual_cols = set(df.columns)

        missing = actual_cols - summary_cols
        extra = summary_cols - actual_cols

        if missing:
            report['errors'].append({
                'check': 'column_names',
                'type': 'missing_columns',
                'message': f"Columns in data but not in summary: {missing}"
            })

        if extra:
            report['errors'].append({
                'check': 'column_names',
                'type': 'extra_columns',
                'message': f"Columns in summary but not in data: {extra}"
            })

        report['checks']['column_names'] = {
            'expected': len(summary_cols),
            'actual': len(actual_cols),
            'match': len(missing) == 0 and len(extra) == 0
        }

    def _check_types(
        self,
        summary: Dict[str, Any],
        df: pd.DataFrame,
        report: Dict[str, Any]
    ) -> None:
        """Check that declared types match actual data."""
        type_mismatches = []

        for col_summary in summary['columns']:
            col_name = col_summary['name']

            if col_name not in df.columns:
                continue

            declared_type = col_summary['type']
            series = df[col_name]

            # Verify numeric type
            if declared_type == 'numeric':
                try:
                    parsed = pd.to_numeric(series.dropna(), errors='coerce')
                    parse_rate = parsed.notna().sum() / len(series.dropna())

                    if parse_rate < self.config['type_agreement_threshold']:
                        type_mismatches.append({
                            'column': col_name,
                            'declared_type': declared_type,
                            'parse_rate': parse_rate,
                            'message': f"Only {parse_rate:.1%} of values parse as numeric"
                        })
                except:
                    type_mismatches.append({
                        'column': col_name,
                        'declared_type': declared_type,
                        'message': "Failed to parse as numeric"
                    })

        if type_mismatches:
            report['errors'].extend([{
                'check': 'type_agreement',
                'type': 'type_mismatch',
                **mismatch
            } for mismatch in type_mismatches])

        report['checks']['type_agreement'] = {
            'mismatches': len(type_mismatches),
            'passed': len(type_mismatches) == 0
        }

    def _check_ranges(
        self,
        summary: Dict[str, Any],
        df: pd.DataFrame,
        report: Dict[str, Any]
    ) -> None:
        """Check for plausible value ranges."""
        range_issues = []

        for col_summary in summary['columns']:
            col_name = col_summary['name']

            if col_name not in df.columns:
                continue

            col_type = col_summary['type']
            col_lower = col_name.lower()

            # Check yields (should be non-negative)
            if col_type == 'numeric' and any(kw in col_lower for kw in ['yield', 'production', 'area']):
                min_val = col_summary.get('min')
                if min_val is not None and min_val < 0:
                    range_issues.append({
                        'column': col_name,
                        'issue': 'negative_values',
                        'min': min_val,
                        'message': f"Found negative values in {col_name} (min: {min_val})"
                    })

            # Check years (should be plausible)
            if 'year' in col_lower:
                min_val = col_summary.get('min')
                max_val = col_summary.get('max')

                if min_val is not None and min_val < self.config['year_min']:
                    range_issues.append({
                        'column': col_name,
                        'issue': 'year_too_old',
                        'min': min_val,
                        'message': f"Year too old: {min_val} < {self.config['year_min']}"
                    })

                if max_val is not None and max_val > self.config['year_max']:
                    range_issues.append({
                        'column': col_name,
                        'issue': 'year_too_future',
                        'max': max_val,
                        'message': f"Year too far in future: {max_val} > {self.config['year_max']}"
                    })

        if range_issues:
            for issue in range_issues:
                report['warnings'].append({
                    'check': 'range_sanity',
                    **issue
                })

        report['checks']['range_sanity'] = {
            'issues_found': len(range_issues),
            'passed': len(range_issues) == 0
        }

    def _check_nulls(
        self,
        summary: Dict[str, Any],
        df: pd.DataFrame,
        report: Dict[str, Any]
    ) -> None:
        """Check for excessive null rates."""
        high_null_cols = []

        for col_summary in summary['columns']:
            col_name = col_summary['name']
            null_rate = col_summary.get('null_rate', 0)

            if null_rate > self.config['null_threshold']:
                high_null_cols.append({
                    'column': col_name,
                    'null_rate': null_rate,
                    'message': f"{col_name} has {null_rate:.1%} null values"
                })

        if high_null_cols:
            for col in high_null_cols:
                report['warnings'].append({
                    'check': 'null_rates',
                    'type': 'high_null_rate',
                    **col
                })

        report['checks']['null_rates'] = {
            'high_null_columns': len(high_null_cols),
            'passed': len(high_null_cols) == 0
        }

    def verify_all(
        self,
        summaries_dir: Path,
        data_dir: Path
    ) -> List[Dict[str, Any]]:
        """
        Verify all summaries in a directory.

        Args:
            summaries_dir: Directory with summary JSON files
            data_dir: Directory with original data files

        Returns:
            List of verification reports
        """
        summaries_dir = Path(summaries_dir)
        data_dir = Path(data_dir)

        summary_files = sorted(summaries_dir.glob("*.summary.json"))

        if not summary_files:
            logger.warning(f"No summary files found in {summaries_dir}")
            return []

        logger.info(f"Verifying {len(summary_files)} summaries")

        reports = []

        for summary_path in summary_files:
            # Find corresponding data file
            file_stem = summary_path.stem.replace('.summary', '')
            data_path = None

            for ext in ['.csv', '.json']:
                candidate = data_dir / f"{file_stem}{ext}"
                if candidate.exists():
                    data_path = candidate
                    break

            if not data_path:
                logger.warning(f"Data file not found for summary: {summary_path.name}")
                continue

            try:
                report = self.verify_summary(summary_path, data_path)
                reports.append(report)
            except Exception as e:
                logger.error(f"Verification failed for {summary_path.name}: {e}")

        # Summary statistics
        passed = sum(1 for r in reports if r['status'] == 'pass')
        failed = sum(1 for r in reports if r['status'] == 'fail')
        warnings = sum(1 for r in reports if r['status'] == 'pass_with_warnings')

        logger.info(f"Verification complete: {passed} passed, {warnings} with warnings, {failed} failed")

        return reports


def main():
    """
    CLI entry point for Verification V1.

    Usage:
        python -m src.verifiers.schema_check
    """
    import argparse
    from utils.file_utils import save_json

    parser = argparse.ArgumentParser(description="Verification V1: Schema Check")
    parser.add_argument(
        '--summaries-dir',
        default='data/summaries',
        help='Directory with summary JSON files'
    )
    parser.add_argument(
        '--data-dir',
        default='data/raw',
        help='Directory with original data files'
    )
    parser.add_argument(
        '--output',
        default='data/summaries/verification_v1.json',
        help='Output path for verification report'
    )

    args = parser.parse_args()

    # Create checker
    checker = SchemaChecker()

    # Run verification
    reports = checker.verify_all(args.summaries_dir, args.data_dir)

    # Save report
    save_json(reports, args.output)

    print(f"\n✓ Verified {len(reports)} summaries")
    print(f"✓ Report saved to: {args.output}")

    # Print summary
    passed = sum(1 for r in reports if r['status'] == 'pass')
    failed = sum(1 for r in reports if r['status'] == 'fail')
    warnings = sum(1 for r in reports if r['status'] == 'pass_with_warnings')

    print(f"\nResults:")
    print(f"  ✓ Passed: {passed}")
    print(f"  ⚠ Warnings: {warnings}")
    print(f"  ✗ Failed: {failed}")


if __name__ == '__main__':
    main()
